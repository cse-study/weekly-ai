+++
title = "Video PreTraining (VPT)"
date = "2022-07-02"
[profile]  
	enable = true  
	avatar = "/assets/img/yuho.jpg"  
	name = "Yuho Jeong"  
	github = "yuhodots"
	email = "yuho8437@unist.ac.kr"

+++

OpenAI의 "Learning to Play Minecraft with Video PreTraining (VPT)" 글을 읽고 내용을 공유합니다.

<!--more-->

### Video PreTraining

- OpenAI에서 Minecraft를 플레이할 수 있는 computer-using agent를 학습시켰음
- 최종적으로는 다이아 곡괭이를 만드는 것을 학습하였는데, 이 과정에서 20분(24000 action) 정도의 사람의 게임 플레이 내용을 unlabeled video dataset 형태로 사용하였고, 플레이어를 고용해서 만든 labeled contractor data가 일부 사용되었음

![img](https://cdn.openai.com/vpt/overview.svg)

<center><p><i>Taken from https://openai.com/blog/vpt/</i></p></center>

1. 인터넷 상에서 70K hour 가량의 unlabeled video 데이터 수집
2. 사람 전문가를 통해 마우스/키보드 액션이 label된 2K hour 가량의 데이터 수집. 이를 통해 비디오 프레임과 마우스/키보드 액션 사이의 관계를 뉴럴넷 모델(**Inverse Dynamics Model, IDM**) 사용하여 학습. 
3. IDM 사용하여 70K unlabeled video 데이터를 라벨링한 뒤에, 해당 데이터를 사용하여 **past frame이 주어지면 future action을 예측하는 VPT Foundation 모델** 학습 (Behavior Cloning, causal)
   - **Input이 past video frames이고, output이 action인 auto-regressive 모델!**
4. Foundation 모델을 획득하면 이 모델을 가지고 zero-shot으로 task를 수행할 수도 있고, fine-tuning하여 모델을 더 task-specific 해지도록 개선할 수도 있음
   - RL 기반 fine-tuning을 할 때는 아래의 sub-tasks를 순차적으로 잘 수행할 때 마다 reward를 제공했다고 함

![img](https://cdn.openai.com/vpt/diamond-pickaxe-sequence.svg)

<center><p><i>Taken from https://openai.com/blog/vpt/</i></p></center>

### References

- [OpenAI, "Learning to Play Minecraft with Video PreTraining (VPT)"](https://openai.com/blog/vpt/)
- [Yannic Kilcher YouTube, "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos (Paper Explained)"](https://www.youtube.com/watch?v=oz5yZc9ULAc)

