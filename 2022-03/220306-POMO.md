+++

title = "POMO: Policy Optimization with Multiple Optima for Reinforcement Learning"
date = "2022-03-06"
[profile]  
	enable = true  
	avatar = "/assets/img/yuho.jpg"  
	name = "Yuho Jeong"  
	github = "yuhodots"
	email = "yuho8437@unist.ac.kr"

+++

Samsung SDS Techtonic의 "POMO: 강화학습을 이용한 조합 최적화(NeurIPS 2020)" 발표를 보고 느낀점을 공유합니다. 해당 자료는 reference로 걸어두었습니다.
<!--more-->

- 조합최적화 문제를 강화학습으로 풀려는 시도들이 최근 많이 있음
- 강화학습을 처음 공부할 때는 강화학습이 풀 수 있는 실제 세상의 문제가 많이 있을지 의문이었는데, 실제 세상에서 적용할 수 있지만 아직 사람들이 가치를 발견하지 못한 경우가 많겠다는 생각이 들었음
- 조합 최적화를 들어보긴 했지만 실제로 어떤 내용인지는 영상을 통해 처음으로 접했음. 내용이 흥미로워서 기초부터 주요 논문까지 쭉 순서대로 제대로 공부해보고 싶다는 생각이 들었음

### References

- [[Techtonic 2020] Track 1. POMO: 강화학습을 이용한 조합 최적화(NeurIPS 2020) - 권영대 프로](https://www.youtube.com/watch?v=Dyp9lQpVgCs)
- [[Techtonic 2021] AI를 사용해 기업의 조합최적화 작업을 처리할 수 있을까? (NeurIPS 2021) - 권영대 프로](https://www.youtube.com/watch?v=hAirBRQSFm0)
- [POMO: Policy Optimization with Multiple Optima for Reinforcement Learning](https://proceedings.neurips.cc/paper/2020/hash/f231f2107df69eab0a3862d50018a9b2-Abstract.html)
